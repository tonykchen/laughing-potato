{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de7e4dca-3653-4bf6-80a0-d964492d1d91",
   "metadata": {},
   "source": [
    "# Train a Pytorch model with a SageMaker Training Job\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e07c4b90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Script mode allows you to build models using a custom algorithm not supported by one of the built-in choices. This is referred to as script mode because you write your custom code (script) in a text file with a .py extension.\n",
    "\n",
    "SageMaker supports most of the popular ML frameworks through pre-built containers, and has taken the extra step to optimize them to work especially well on AWS compute and network infrastructure in order to achieve near-linear scaling efficiency. These pre-built containers also provide some additional Python packages, such as Pandas and NumPy, so you can write your own code for training an algorithm. These frameworks also allow you to install any Python package hosted on PyPi by including a requirements.txt file with your training code or to include your own code directories.\n",
    "\n",
    "In this example, we will train a PyTorch MNIST model using a SageMaker Training Job.\n",
    "\n",
    "\n",
    "**Note**: Select the image `PyTorch 1.12 Python 3.8 CPU Optimized image` and instance type `ml.c5.large`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3368d208-aebb-4844-bf27-2b2e373ef3d2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037c2813-b191-4420-b37b-9c6d1cbb8057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.session import Session\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "sm_session = Session()\n",
    "region = sm_session.boto_session.region_name\n",
    "default_bucket = sm_session.default_bucket()\n",
    "print(default_bucket)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b042521b-f8b1-40a4-804d-b2cb811d79f3",
   "metadata": {},
   "source": [
    "We will make the MNIST data accessible to our training job by uploading it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae766a72-e6d3-4a65-b147-682a63430c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_prefix = \"mnist_training_job_examples\"\n",
    "data_prefix = \"data\"\n",
    "data_s3_uri = sm_session.upload_data(path=\"mnist_data\", bucket=default_bucket, key_prefix=f\"{base_prefix}/{data_prefix}\")\n",
    "print(data_s3_uri) # S3 location of the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c82b5573-32ec-440d-89ce-709e285dce19",
   "metadata": {},
   "source": [
    "Let's confirm that the data is in S3 using the AWS CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2cbdd-5d22-440b-899e-d2cb07b93295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! aws s3 ls {data_s3_uri}/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9dc0054-d7dd-4ec8-b1e9-0b292fc7b1c0",
   "metadata": {},
   "source": [
    "## Create model training script\n",
    "Let's create `mnist.py`, the pytorch script file to train our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49259885-530e-4675-bd72-e21934014e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c6e08a-92d3-4819-a080-4858337813cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./script/mnist.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "from os.path import join\n",
    "import boto3\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "# Based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, kernel_size, drop_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, hidden_channels, kernel_size=kernel_size)\n",
    "        self.conv2 = torch.nn.Conv2d(hidden_channels, 20, kernel_size=kernel_size)\n",
    "        self.conv2_drop = torch.nn.Dropout2d(p=drop_out)\n",
    "        self.fc1 = torch.nn.Linear(320, 50)\n",
    "        self.fc2 = torch.nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(torch.nn.functional.max_pool2d(self.conv1(x), 2))\n",
    "        x = torch.nn.functional.relu(\n",
    "            torch.nn.functional.max_pool2d(self.conv2_drop(self.conv2(x)), 2)\n",
    "        )\n",
    "        x = x.view(-1, 320)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return torch.nn.functional.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def log_performance(model, data_loader, device, epoch, metric_type=\"Test\"):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss += torch.nn.functional.nll_loss(\n",
    "                output, target, reduction=\"sum\"\n",
    "            ).item()  # sum up batch loss\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.0 * correct / len(data_loader.dataset)\n",
    "    # log metrics\n",
    "    logger.info(\n",
    "        \"{} Average loss: {:.4f}, {} Accuracy: {:.4f}%;\\n\".format(\n",
    "            metric_type, loss, metric_type, accuracy\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    train_set, test_set, optimizer=\"sgd\", epochs=10, hidden_channels=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Function that trains the CNN classifier to identify the MNIST digits.\n",
    "    Args:\n",
    "        train_set (torchvision.datasets.mnist.MNIST): train dataset\n",
    "        test_set (torchvision.datasets.mnist.MNIST): test dataset\n",
    "        optimizer (str): the optimization algorthm to use for training your CNN\n",
    "                         available options are sgd and adam\n",
    "        epochs (int): number of complete pass of the training dataset through the algorithm\n",
    "        hidden_channels (int): number of hidden channels in your model\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # set the seed for generating random numbers\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=1000, shuffle=True)\n",
    "    logger.info(\n",
    "        \"Processes {}/{} ({:.0f}%) of train data\".format(\n",
    "            len(train_loader.sampler),\n",
    "            len(train_loader.dataset),\n",
    "            100.0 * len(train_loader.sampler) / len(train_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        \"Processes {}/{} ({:.0f}%) of test data\".format(\n",
    "            len(test_loader.sampler),\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * len(test_loader.sampler) / len(test_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "    model = Net(hidden_channels, kernel_size=5, drop_out=0.5).to(device)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    momentum = 0.5\n",
    "    lr = 0.01\n",
    "    log_interval = 100\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(\"Training Epoch:\", epoch)\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = torch.nn.functional.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                logger.info(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)], Train Loss: {:.6f};\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.sampler),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "        log_performance(model, train_loader, device, epoch, \"Train\")\n",
    "        log_performance(model, test_loader, device, epoch, \"Test\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model(model, model_dir):\n",
    "    logger.info(\"Saving the model.\")\n",
    "    path = os.path.join(model_dir, \"model.pth\")\n",
    "    # recommended way from http://pytorch.org/docs/master/notes/serialization.html\n",
    "    torch.save(model.cpu().state_dict(), path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        metavar=\"N\",\n",
    "        help=\"number of epochs to train (default: 10)\",\n",
    "    )\n",
    "    parser.add_argument(\"--optimizer\", type=str, default=\"sgd\", help=\"optimizer for training.\")\n",
    "    parser.add_argument(\n",
    "        \"--hidden_channels\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"number of channels in hidden conv layer\",\n",
    "    )\n",
    "\n",
    "    # Container environment\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--num-gpus\", type=int, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "    parser.add_argument(\"--data-dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAINING\"])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    train_set = datasets.MNIST(\n",
    "        args.data_dir,\n",
    "        train=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        ),\n",
    "        download=False,\n",
    "    )\n",
    "\n",
    "    test_set = datasets.MNIST(\n",
    "        args.data_dir,\n",
    "        train=False,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        ),\n",
    "        download=False,\n",
    "    )\n",
    "    \n",
    "    model = train_model(\n",
    "        train_set,\n",
    "        test_set,\n",
    "        optimizer=args.optimizer,\n",
    "        epochs=args.epochs,\n",
    "        hidden_channels=args.hidden_channels,\n",
    "        )\n",
    "    save_model(model, args.model_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2199b07a-cc85-454b-b9e1-9815fb30f482",
   "metadata": {},
   "source": [
    "As you can see above, the training script is very similar to a training script you might run outside of SageMaker. During the training job, you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string representing the container directory `/opt/ml/model`. This is the directory where you can save your model outputs during training. When your training job completes, SageMaker will package the directory contents into a compressed `tar` archive and save it to a sepecified S3 location.\n",
    "\n",
    "* `SM_CHANNEL_[channel_name]`: A string representing the path to the directory containing the channel data. In the script below,`SM_CHANNEL_TRAINING` represents the directory `/opt/ml/input/data/training`, which contains data for the `\"training\"` channel.\n",
    "\n",
    "**Note:** It is also possible to specify additional channels, such as `\"testing\"`. This would create the environment variable `SM_CHANNEL_TESTING` which points to `/opt/ml/input/data/testing`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc1913d9-fe5f-4cf1-aca6-c6f6c12bd21c",
   "metadata": {},
   "source": [
    "## Start the Training Job\n",
    "\n",
    "First, create a [PyTorch estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html) with the following:\n",
    "\n",
    "* `source_dir`: Path to a local directory that contains the training script. This directory can also contain other Python modules the script depends as well as a `requirements.txt` file for specifying additional dependencies to install. SageMaker will copy all of the files under this directory to the container directory `/opt/ml/code`.\n",
    "\n",
    "* `entry_point`: The training script which SageMaker will execute. This file should exist in `source_dir`.\n",
    "\n",
    "* `role`: The ARN for the [SageMaker execution role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).\n",
    "\n",
    "* `output_path`: A S3 path to save model artifacts to.\n",
    "\n",
    "* `framework_version`: PyTorch version you want to use for executing your model training code.\n",
    "\n",
    "* `py_version`: Python version you want to use for executing your model training code.\n",
    "\n",
    "* `instance_type`: The instance type to use for training.\n",
    "\n",
    "* `instance_count`: Number of instances you want to use for training.\n",
    "\n",
    "* `hyperparameters`: The hyperparameters which will be used for training. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance.\n",
    "\n",
    "* `input_mode`: How the training data is made available to the training container. When this is set as \"File\" (the default), SageMaker will download all of the data from S3 the EBS volume before training. For the complete set of options for making your data available, please visit the documentation for accessing training data [here](https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html).\n",
    "\n",
    "* `volume_size`: The size of the EBS volume for the training job. This should be large enough to hold the training data which will be downloaded from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f266e0-d73d-452c-a3ed-51b0fc48075d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    source_dir=\"script\",\n",
    "    entry_point=\"mnist.py\",\n",
    "    role=role,\n",
    "    output_path=f\"s3://{default_bucket}/{base_prefix}/models\",\n",
    "    framework_version=\"1.12\",\n",
    "    py_version=\"py38\",\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    instance_count=1,\n",
    "    hyperparameters={\"epochs\": 10, \"hidden_channels\": 5, \"optimizer\": \"adam\"},\n",
    "    input_mode=\"File\",\n",
    "    volume_size=10\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4e84a78-a158-4a77-ae48-4584cb4c5e47",
   "metadata": {},
   "source": [
    "Using the `fit()` method, we supply one input channel, `\"training\"`, and map it to our data in S3. Since the `input_mode` is set to `\"File\"`, SageMaker will download all of the data to `/opt/ml/input/data/training` before training begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e3d465-5ef5-4fe3-97d5-fd793bf8aaa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator.fit({\"training\": data_s3_uri}, job_name=\"my-first-training-job\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7f62a22-f882-4115-8d7e-a3a6d342c234",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "We will specify the hyperparameters we want to tune and their possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5559496-0568-4077-aae3-fb15a60cf87b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import (\n",
    "    CategoricalParameter,\n",
    "    IntegerParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"hidden_channels\": IntegerParameter(5, 15, scaling_type='Auto'),\n",
    "    \"optimizer\": CategoricalParameter([\"adam\", \"sgd\"])\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "231a11af-45e0-4c1e-a9c0-03606aca8daf",
   "metadata": {},
   "source": [
    "Next we'll specify the objective metric that we'd like to tune and its definition, which includes the regular expression (Regex) needed to extract that metric from the CloudWatch logs of the training job. In this particular case, our script emits average loss value and we will use it as the objective metric, we also set the objective_type to be 'minimize', so that hyperparameter tuning seeks to minize the objective metric when searching for the best hyperparameter setting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b5bfa-4c5e-494e-ab92-f0c942d170cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "objective_metric_name = \"average test loss\"\n",
    "objective_type = \"Minimize\"\n",
    "metric_definitions = [{\"Name\": \"average test loss\", \"Regex\": \"Test Average loss: ([0-9\\\\.]+)\"}]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00588aeb-f4e5-43e8-96f9-0d77e56c4024",
   "metadata": {},
   "source": [
    "We will create a HyperparameterTuner object, to which we pass:\n",
    "\n",
    "* The PyTorch estimator we created above\n",
    "* Our hyperparameter ranges\n",
    "* Objective metric name and definition\n",
    "* Tuning resource configurations such as Number of training jobs to run in total and how many training jobs can be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca57a5-b311-4962-a1f4-04595bbafdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    max_jobs=4,\n",
    "    max_parallel_jobs=4,\n",
    "    objective_type=objective_type,\n",
    "    strategy='Random'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97d797fc-1cab-4e55-b736-7973215fe811",
   "metadata": {},
   "source": [
    "We start our hyperprameter tuning job by calling .fit() and passing in the S3 path to our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c478d6f0-f8b5-4784-9464-8ca4a5498cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({\"training\": data_s3_uri}, wait=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2175ba3c-9fd4-4941-80ce-738f4b71a48b",
   "metadata": {},
   "source": [
    "We can view the results in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb97004-87ee-428d-9e1e-82a86946f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_analytis = tuner.analytics()\n",
    "tuner_analytis.dataframe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93c354f9-251f-4f38-81dc-567e86f13336",
   "metadata": {},
   "source": [
    "### Optional\n",
    "\n",
    "Store variables to use in the lab `optional/pytorch_training_job_experiment.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031259e4-8f45-4fee-bfc0-e39263811148",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store base_prefix\n",
    "%store data_s3_uri"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
